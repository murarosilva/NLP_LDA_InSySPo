{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from gensim import models\n",
    "import pprint\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pprint import pprint\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "my_stopwords = {'approach','paper','article','model','viewpoint','conclusion','discussion','method','introduction','section','framework','based','result','proposed','theory','information','process','analysis','data','problem','review','springer','verlag'}\n",
    "my_stopwords = set.union(stopwords,my_stopwords)\n",
    "my_stopwords = frozenset(my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scopus (8)_cn.ris', 'scopus (9)_us.ris', 'scopus (14)_us.ris', 'scopus (1)_cn.ris', 'scopus (15)_us.ris', 'scopus (8)_us.ris', 'scopus_br.ris', 'scopus (1)_us.ris', 'scopus (9)_cn.ris', 'scopus (2)_cn.ris', 'scopus (3)_us.ris', 'scopus (2)_us.ris', 'scopus (16)_us.ris', 'scopus (3)_cn.ris', 'scopus (6)_cn.ris', 'scopus (13)_us.ris', 'scopus (12)_cn.ris', 'scopus (7)_us.ris', 'scopus (6)_us.ris', 'scopus (2)_br.ris', 'scopus (7)_cn.ris', 'scopus (12)_us.ris', 'scopus (11)_cn.ris', 'scopus (4)_us.ris', 'scopus_cn.ris', 'scopus (5)_cn.ris', 'scopus (10)_us.ris', 'scopus (4)_cn.ris', 'scopus (11)_us.ris', 'scopus (10)_cn.ris', 'scopus (1)_br.ris', 'scopus_us.ris', 'scopus (5)_us.ris']\n"
     ]
    }
   ],
   "source": [
    "data_folder = '../new_data/'\n",
    "files = os.listdir(data_folder)\n",
    "files.remove('.DS_Store') #remove arquivo criado automaticamente sei lá porquê\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "dois = []\n",
    "author_address = []\n",
    "publication_year = []\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pensar em tirar duplicatas (comparando pelo título)\n",
    "for ff in files: \n",
    "    f = data_folder + ff\n",
    "    fid = open(f,'r',encoding='utf8')\n",
    "    data = readris(fid)\n",
    "\n",
    "    for i,entry in enumerate(data):\n",
    "    \n",
    "        try:\n",
    "            title = entry['title']\n",
    "            abstract = entry['abstract']\n",
    "            \n",
    "            if 'doi' in entry:\n",
    "                dois += [entry['doi']]\n",
    "            else:\n",
    "                dois += [['']]\n",
    "\n",
    "            if 'author_address' in entry:\n",
    "                country = entry['author_address'].split(',')[-1]\n",
    "                if country[0] == ' ':\n",
    "                    country = country[1:]\n",
    "                author_address += [country]\n",
    "            else:\n",
    "                author_address += ['-']\n",
    "\n",
    "            if 'publication_year' in entry:\n",
    "                publication_year += [entry['publication_year']]\n",
    "            else:\n",
    "                publication_year += ['-']\n",
    "\n",
    "            text = title + ' ' + abstract\n",
    "            text = gensim.utils.simple_preprocess(text)\n",
    "            for i,t in enumerate(text):\n",
    "                text[i] = lem.lemmatize(t)\n",
    "\n",
    "            text = [word for word in text if word not in my_stopwords]\n",
    "\n",
    "            texts += [text]\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    fid.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.018*\"fuzzy\" + 0.017*\"set\" + 0.012*\"graph\" + 0.011*\"rule\" + 0.007*\"knowledge\" + 0.007*\"function\" + 0.007*\"reasoning\" + 0.007*\"concept\" + 0.007*\"relation\" + 0.006*\"structure\"\n",
      "Topic: 1 \n",
      "Words: 0.016*\"material\" + 0.016*\"machine\" + 0.012*\"learning\" + 0.011*\"ml\" + 0.010*\"structure\" + 0.009*\"chemical\" + 0.008*\"design\" + 0.008*\"property\" + 0.007*\"artificial\" + 0.007*\"intelligence\"\n",
      "Topic: 2 \n",
      "Words: 0.036*\"agent\" + 0.036*\"human\" + 0.014*\"intelligence\" + 0.013*\"artificial\" + 0.012*\"behavior\" + 0.010*\"ai\" + 0.008*\"interaction\" + 0.008*\"autonomous\" + 0.007*\"environment\" + 0.007*\"cognitive\"\n",
      "Topic: 3 \n",
      "Words: 0.025*\"wa\" + 0.023*\"patient\" + 0.011*\"clinical\" + 0.010*\"learning\" + 0.010*\"study\" + 0.010*\"diagnosis\" + 0.009*\"accuracy\" + 0.008*\"performance\" + 0.007*\"sensitivity\" + 0.007*\"algorithm\"\n",
      "Topic: 4 \n",
      "Words: 0.069*\"network\" + 0.039*\"neural\" + 0.011*\"artificial\" + 0.011*\"time\" + 0.009*\"learning\" + 0.008*\"control\" + 0.008*\"algorithm\" + 0.008*\"deep\" + 0.006*\"performance\" + 0.006*\"prediction\"\n",
      "Topic: 5 \n",
      "Words: 0.016*\"research\" + 0.010*\"user\" + 0.009*\"social\" + 0.009*\"science\" + 0.008*\"study\" + 0.008*\"intelligence\" + 0.007*\"artificial\" + 0.006*\"text\" + 0.006*\"ha\" + 0.005*\"knowledge\"\n",
      "Topic: 6 \n",
      "Words: 0.024*\"fault\" + 0.018*\"language\" + 0.017*\"task\" + 0.014*\"diagnosis\" + 0.009*\"natural\" + 0.009*\"gas\" + 0.009*\"knowledge\" + 0.009*\"processing\" + 0.009*\"production\" + 0.008*\"wa\"\n",
      "Topic: 7 \n",
      "Words: 0.016*\"water\" + 0.012*\"wa\" + 0.010*\"study\" + 0.007*\"soil\" + 0.006*\"area\" + 0.006*\"prediction\" + 0.005*\"artificial\" + 0.005*\"support\" + 0.005*\"forest\" + 0.005*\"spatial\"\n",
      "Topic: 8 \n",
      "Words: 0.057*\"algorithm\" + 0.031*\"optimization\" + 0.013*\"search\" + 0.010*\"optimal\" + 0.010*\"solution\" + 0.009*\"ant\" + 0.008*\"time\" + 0.008*\"colony\" + 0.008*\"performance\" + 0.008*\"swarm\"\n",
      "Topic: 9 \n",
      "Words: 0.056*\"image\" + 0.016*\"feature\" + 0.013*\"segmentation\" + 0.011*\"recognition\" + 0.009*\"algorithm\" + 0.009*\"object\" + 0.008*\"classification\" + 0.006*\"video\" + 0.006*\"region\" + 0.005*\"imaging\"\n",
      "Topic: 10 \n",
      "Words: 0.026*\"decision\" + 0.014*\"risk\" + 0.012*\"support\" + 0.011*\"health\" + 0.009*\"patient\" + 0.009*\"management\" + 0.008*\"making\" + 0.007*\"care\" + 0.006*\"time\" + 0.005*\"ha\"\n",
      "Topic: 11 \n",
      "Words: 0.058*\"learning\" + 0.021*\"machine\" + 0.017*\"feature\" + 0.015*\"classification\" + 0.013*\"algorithm\" + 0.010*\"performance\" + 0.008*\"sample\" + 0.008*\"training\" + 0.008*\"classifier\" + 0.007*\"class\"\n",
      "Topic: 12 \n",
      "Words: 0.041*\"ai\" + 0.027*\"intelligence\" + 0.026*\"artificial\" + 0.015*\"technology\" + 0.011*\"research\" + 0.011*\"application\" + 0.010*\"medical\" + 0.009*\"ha\" + 0.009*\"development\" + 0.008*\"clinical\"\n",
      "Topic: 13 \n",
      "Words: 0.015*\"detection\" + 0.009*\"image\" + 0.007*\"algorithm\" + 0.007*\"pressure\" + 0.007*\"learning\" + 0.007*\"wa\" + 0.007*\"ct\" + 0.006*\"diagnosis\" + 0.006*\"cardiac\" + 0.006*\"time\"\n",
      "Topic: 14 \n",
      "Words: 0.016*\"student\" + 0.014*\"wa\" + 0.009*\"artificial\" + 0.008*\"intelligence\" + 0.007*\"ecg\" + 0.007*\"study\" + 0.006*\"program\" + 0.005*\"ai\" + 0.005*\"use\" + 0.005*\"human\"\n",
      "Topic: 15 \n",
      "Words: 0.015*\"cancer\" + 0.014*\"drug\" + 0.014*\"gene\" + 0.010*\"cell\" + 0.008*\"disease\" + 0.008*\"treatment\" + 0.008*\"knowledge\" + 0.008*\"tumor\" + 0.007*\"study\" + 0.006*\"expression\"\n",
      "Topic: 16 \n",
      "Words: 0.020*\"technology\" + 0.016*\"intelligence\" + 0.013*\"design\" + 0.013*\"artificial\" + 0.013*\"intelligent\" + 0.013*\"development\" + 0.012*\"application\" + 0.010*\"research\" + 0.009*\"service\" + 0.008*\"ha\"\n",
      "Topic: 17 \n",
      "Words: 0.022*\"sensor\" + 0.015*\"energy\" + 0.014*\"device\" + 0.010*\"artificial\" + 0.010*\"power\" + 0.009*\"high\" + 0.008*\"intelligence\" + 0.008*\"application\" + 0.007*\"sensing\" + 0.007*\"signal\"\n",
      "Topic: 18 \n",
      "Words: 0.014*\"robot\" + 0.012*\"knowledge\" + 0.010*\"planning\" + 0.009*\"game\" + 0.008*\"control\" + 0.008*\"domain\" + 0.006*\"plan\" + 0.006*\"action\" + 0.006*\"reasoning\" + 0.006*\"time\"\n",
      "Topic: 19 \n",
      "Words: 0.021*\"detection\" + 0.019*\"prediction\" + 0.017*\"protein\" + 0.015*\"feature\" + 0.011*\"accuracy\" + 0.009*\"sequence\" + 0.008*\"machine\" + 0.007*\"set\" + 0.007*\"wa\" + 0.006*\"wind\"\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=4, workers=4)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "scores = []\n",
    "for ind_doc, bc in enumerate(bow_corpus):\n",
    "    ind_top, score = sorted(lda_model[bc], key=lambda tup: -1*tup[1])[0]\n",
    "    topics += [ind_top]\n",
    "    scores += [score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5226966"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_doc = 6\n",
    "topics[ind_doc]\n",
    "scores[ind_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title & Abs: ['salient', 'environmental', 'sound', 'detection', 'machine', 'awareness', 'auditory', 'perception', 'essential', 'environment', 'perception', 'saliency', 'detection', 'fundamental', 'basis', 'efficient', 'way', 'achieving', 'task', 'artificial', 'machine', 'intelligent', 'perception', 'sound', 'required', 'provide', 'awareness', 'initiatory', 'step', 'artificial', 'consciousness', 'novel', 'salient', 'environment', 'sound', 'detection', 'machine', 'awareness', 'heterogeneous', 'saliency', 'feature', 'image', 'acoustic', 'channel', 'improve', 'efficiency', 'global', 'informative', 'saliency', 'estimation', 'initially', 'short', 'term', 'shannon', 'entropy', 'series', 'auditory', 'saliency', 'detection', 'presented', 'obtain', 'spectral', 'temporal', 'saliency', 'feature', 'power', 'spectral', 'density', 'mel', 'frequency', 'cepstral', 'coefficient', 'respectively', 'computational', 'bio', 'inspired', 'inhibition', 'return', 'saliency', 'verification', 'improve', 'accuracy', 'detection', 'heterogeneous', 'saliency', 'feature', 'fusion', 'introduced', 'form', 'final', 'auditory', 'saliency', 'map', 'combining', 'acoustic', 'image', 'saliency', 'feature', 'environmental', 'sound', 'collected', 'real', 'world', 'applied', 'verify', 'superiority', 'effective', 'detection', 'overlapped', 'salient', 'sound', 'robust', 'background', 'noise', 'compared', 'conventional', 'elsevier']\\Country: China\n",
      "DOI:10.1016/j.neucom.2014.09.046\n",
      "Topic: 9 - Score: 0.5623683929443359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teste = 450\n",
    "print('\\nTitle & Abs: {}\\Country: {}\\nDOI:{}\\nTopic: {} - Score: {}\\n'.format(texts[teste], author_address[teste], dois[teste], topics[teste], scores[teste]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53816 53816 53816 53816\n"
     ]
    }
   ],
   "source": [
    "print(len(topics),len(author_address),len(texts),len(dois))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.507760226726532\t \n",
      "Topic (7): 0.044*\"image\" + 0.021*\"recognition\" + 0.015*\"feature\" + 0.012*\"object\" + 0.008*\"proposed\" + 0.008*\"pattern\" + 0.008*\"visual\" + 0.007*\"shape\" + 0.007*\"algorithm\" + 0.007*\"motion\"\n",
      "\n",
      "Score: 0.3839522898197174\t \n",
      "Topic (17): 0.035*\"learning\" + 0.030*\"data\" + 0.020*\"algorithm\" + 0.018*\"classification\" + 0.018*\"feature\" + 0.012*\"proposed\" + 0.010*\"set\" + 0.010*\"machine\" + 0.010*\"classifier\" + 0.010*\"method\"\n",
      "\n",
      "Score: 0.03924466669559479\t \n",
      "Topic (2): 0.028*\"rule\" + 0.021*\"logic\" + 0.020*\"reasoning\" + 0.018*\"theory\" + 0.016*\"fuzzy\" + 0.015*\"system\" + 0.010*\"representation\" + 0.010*\"knowledge\" + 0.009*\"inference\" + 0.009*\"model\"\n",
      "\n",
      "Score: 0.03518419712781906\t \n",
      "Topic (4): 0.055*\"network\" + 0.017*\"detection\" + 0.017*\"data\" + 0.014*\"fault\" + 0.013*\"sensor\" + 0.011*\"neural\" + 0.011*\"proposed\" + 0.011*\"time\" + 0.009*\"monitoring\" + 0.007*\"learning\"\n",
      "\n",
      "Score: 0.027981441468000412\t \n",
      "Topic (3): 0.022*\"human\" + 0.013*\"health\" + 0.011*\"intelligence\" + 0.011*\"artificial\" + 0.009*\"robot\" + 0.007*\"care\" + 0.007*\"study\" + 0.007*\"cognitive\" + 0.006*\"covid\" + 0.006*\"social\"\n"
     ]
    }
   ],
   "source": [
    "ind_doc = 430\n",
    "for index, score in sorted(lda_model[bow_corpus[ind_doc]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic ({}): {}\".format(score, index, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bow_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-fb2a39be5e85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bow_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
